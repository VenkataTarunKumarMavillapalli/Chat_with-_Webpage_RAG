## ğŸŒ Chat with Webpage using Llama3 and RAG
- This project provides an interactive Streamlit app that enables users to query and interact with the content of a webpage using a Retrieval-Augmented Generation (RAG) setup and the local llama3.2 model.

### âœ¨ Features
- ğŸ› ï¸ Webpage Loader: Extracts content from a user-provided webpage.
- ğŸ” Text Splitter: Divides webpage content into manageable chunks for embedding and retrieval.
- ğŸ“¦ Vector Store: Utilizes Chroma for storing and retrieving embeddings.
- ğŸ¤– Language Model Integration: Uses Ollama's llama3.2 model to answer user queries based on retrieved webpage content.

### ğŸ—‚ï¸ Project Structure

#### ğŸ”‘ Core Components
- ğŸ›ï¸ Streamlit Interface: Provides a user-friendly interface for loading a webpage and asking questions.
- ğŸŒ WebBaseLoader: Loads content from a webpage.
- âœ‚ï¸ Text Splitter: Splits the webpage content into smaller chunks for effective retrieval.
- ğŸ“š Vector Store: Stores and retrieves content embeddings.
- ğŸ¤ Ollama Integration: Leverages the llama3.2 model for generating responses.

### ğŸ§‘â€ğŸ’» Usage

- Provide Webpage URL: ğŸŒ Enter the URL of the webpage you want to analyze in the input field.
- Load Webpage Content: ğŸ“¥ The app will load the content of the webpage and preprocess it.
- Ask Questions: â“ Input any question related to the webpageâ€™s content in the provided text field.
- View Responses: âœ¨ The app will generate a response using the llama3.2 model, augmented by the retrieved context.

### ğŸ§© Components in Detail

##### ğŸŒ Webpage Loading
- Module: WebBaseLoader
- Description: Fetches the HTML content of the provided webpage URL.

##### âœ‚ï¸ Text Splitting
- Module: RecursiveCharacterTextSplitter
- Description: Splits the webpage content into chunks of 500 characters with a 10-character overlap for better embedding and retrieval.

##### ğŸ“š Embeddings and Vector Store
- Embeddings: Generated using OllamaEmbeddings with the llama3.2 model.
- Vector Store: Content embeddings are stored and queried using Chroma.

##### ğŸ¤– Llama3 Integration
- Model: llama3.2
- Integration: Uses ollama.chat() to interact with the language model.
- Context: Combines retrieved content chunks to provide context for generating responses.

##### ğŸ”„ RAG Setup
- Retriever: Queries the vector store to retrieve the most relevant content.
- Chain: Combines retrieved content into a formatted context and queries the language model.

##### ğŸ› ï¸ Example Workflow
- ğŸŒ Enter the URL: https://example.com
- â“ Ask a Question: "What is this webpage about?"
- âœ¨ View Response: The app fetches content, retrieves relevant context, and provides an intelligent answer.

